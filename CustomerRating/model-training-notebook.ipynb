{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy a Neural Collaborative Filtering Model\n",
    "\n",
    "In this notebook, you will execute code blocks to\n",
    "\n",
    "1. inspect the training script [ncf.py](./ncf.py)  \n",
    "2. train a model using [Tensorflow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html)  \n",
    "3. deploy and host the trained model as an endpoint using Amazon SageMaker Hosting Services  \n",
    "4. perform batch inference by calling the model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#################\n",
    "## Code Cell 1 ##\n",
    "#################\n",
    "\n",
    "# Install some required libraries\n",
    "!pip install -U sagemaker\n",
    "!pip install tensorflow==2.6.2\n",
    "!pip install dynamo-pandas[boto3]\n",
    "\n",
    "# Restore the previously stored unique products data frame\n",
    "import pandas as pd\n",
    "import pickle\n",
    "df_unique_products = pd.read_pickle('products.pkl')\n",
    "df_unique_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 2 ##\n",
    "#################\n",
    "\n",
    "# In the last notebook (data-preparation-notebook.ipynb), we stored two variables.\n",
    "# Let's restore those variables here. These variables are inputs for the model training process.\n",
    "\n",
    "# Import some required libraries.\n",
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# Print version numbers\n",
    "logger.info(f\"TesnorFlow version:{tf.__version__}\")\n",
    "logger.info(f'[Using SageMaker version: {sagemaker.__version__}]')\n",
    "\n",
    "# Get stored variables\n",
    "%store -r n_customer\n",
    "%store -r n_product\n",
    "\n",
    "print(n_customer)\n",
    "print(n_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 3 ##\n",
    "#################\n",
    "\n",
    "# import additional required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.tensorflow.serving import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"execution role ARN:\", role)\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "print(\"default bucket name:\", bucket_name)\n",
    "prefix = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 4 ##\n",
    "#################\n",
    "\n",
    "# specify the location of the training data we uploaded to S3 from the previous notebook\n",
    "training_data_uri = os.path.join(f's3://{bucket_name}', 'data')\n",
    "\n",
    "print(training_data_uri)\n",
    "\n",
    "# inspect the training script using `pygmentize` magic\n",
    "!pygmentize 'ncf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 5 ##\n",
    "#################\n",
    "\n",
    "# specify training instance type and model hyperparameters\n",
    "# note that for the demo purpose, the number of epoch is set to 1\n",
    "\n",
    "num_of_instance = 1                 # number of instance to use for training\n",
    "instance_type = 'ml.m5.2xlarge'     # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256, \n",
    "    'n_user': n_customer, \n",
    "    'n_item': n_product\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.6.2'\n",
    "python_version = 'py38'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 6 ##\n",
    "#################\n",
    "\n",
    "# initiate the training job using Tensorflow estimator\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    instance_count=num_of_instance,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distribution=distributed_training_spec,\n",
    "    hyperparameters=training_parameters\n",
    ")\n",
    "\n",
    "# kick off the training job\n",
    "ncf_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Endpoint to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 7 ##\n",
    "#################\n",
    "\n",
    "# Once the model is trained, we can deploy the model using Amazon SageMaker Hosting Services\n",
    "# Here we deploy the model using one ml.m5.2xlarge instance as a tensorflow-serving endpoint\n",
    "# This enables us to invoke the endpoint like how we use Tensorflow serving\n",
    "# Read more about Tensorflow serving using the link below\n",
    "# https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\n",
    "\n",
    "\n",
    "# Upload the model to Amazon S3\n",
    "tf_model = ncf_estimator.model_data\n",
    "output = f's3://{bucket_name}/{prefix}/model.tar.gz'\n",
    "!aws s3 cp {tf_model} {output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 8 ##\n",
    "#################\n",
    "\n",
    "IMAGE_URI = '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.6-gpu-py38-cu112-ubuntu20.04-v1'\n",
    "model_data_prefix = f's3://{bucket_name}/{prefix}/'\n",
    "\n",
    "model = TensorFlowModel(\n",
    "    model_data=output,\n",
    "    role=role,\n",
    "    image_uri=IMAGE_URI\n",
    ")\n",
    "\n",
    "instance_type = 'ml.m5.2xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the Endpoint by passing it some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "## Code Cell 9 ##\n",
    "#################\n",
    "\n",
    "# Define a function to read testing data\n",
    "def _load_testing_data(base_dir):\n",
    "    \"\"\" load testing data \"\"\"\n",
    "    df_test = np.load(os.path.join(base_dir, 'test.npy'),allow_pickle=True)\n",
    "    user_test, item_test, y_test = np.split(np.transpose(df_test).flatten(), 3)\n",
    "    return user_test, item_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## Code Cell 10 ##\n",
    "##################\n",
    "\n",
    "# read testing data from local\n",
    "user_test, item_test, test_labels = _load_testing_data('./data/')\n",
    "\n",
    "# one-hot encode the testing data for model input\n",
    "test_user_data = tf.one_hot(user_test, depth=n_customer).numpy().tolist()\n",
    "test_item_data = tf.one_hot(item_test, depth=n_product).numpy().tolist()\n",
    "    \n",
    "# if you're using Tensorflow 2.0 for one hot encoding\n",
    "# you can convert the tensor to list using:\n",
    "# tf.one_hot(uuser_test, depth=n_user).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## Code Cell 11 ##\n",
    "##################\n",
    "\n",
    "# make batch prediction\n",
    "batch_size = 100\n",
    "y_pred = []\n",
    "for idx in range(0, len(test_user_data), batch_size):\n",
    "    # reformat test samples into tensorflow serving acceptable format\n",
    "    input_vals = {\n",
    "     \"instances\": [\n",
    "         {'input_1': u, 'input_2': i} \n",
    "         for (u, i) in zip(test_user_data[idx:idx+batch_size], test_item_data[idx:idx+batch_size])\n",
    "    ]}\n",
    " \n",
    "    # invoke model endpoint to make inference\n",
    "    pred = predictor.predict(input_vals)\n",
    "    \n",
    "    # store predictions\n",
    "    y_pred.extend([i[0] for i in pred['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## Code Cell 12 ##\n",
    "##################\n",
    "\n",
    "# let's see some prediction examples, assuming the threshold of 0.5\n",
    "# --- prediction probability view ---\n",
    "print('This is what the raw prediction output looks like')\n",
    "print(y_pred[:5],end='\\n\\n\\n')\n",
    "\n",
    "# --- user item pair prediction view, with threshold of 0.5 applied ---\n",
    "pred_df = pd.DataFrame([\n",
    "    user_test,\n",
    "    item_test,\n",
    "    (np.array(y_pred) >= 0.5).astype(int)],\n",
    ").T\n",
    "\n",
    "# Add column headers to the data frame\n",
    "pred_df.columns = ['CustomerId', 'ProductId', 'prediction']\n",
    "\n",
    "print('We can convert the output to user-item pair as shown below, and add back the ProductName data')\n",
    "df_combined = pd.merge(pred_df, df_unique_products, how='inner',on='ProductId')\n",
    "display(df_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the recommendations dataframe to the DynamoDB customers table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## Code Cell 13 ##\n",
    "##################\n",
    "\n",
    "# Useful functions to interact with DynamoDB directly.\n",
    "from dynamo_pandas import put_df, get_df, keys\n",
    "\n",
    "# Create a new Pandas dataframe grouping by CustomerId; Include only recommended products\n",
    "\n",
    "## DIY Note ##\n",
    "## Initially load the recommendations to DynamoDB as ProductIds. In the DIY section \n",
    "## of the lab you will be required to edit the below line and apply the (list) function to the ProductName column.\n",
    "recommend_df = df_combined.query('prediction == 1').groupby('CustomerId').ProductId.apply(list).to_frame().reset_index()\n",
    "\n",
    "# Rename column ProductId to recommendations. When using put_df helper function, the\n",
    "# columns names of the dataframe and dynamoDB table must match verbatim.\n",
    "\n",
    "## DIY Note ##\n",
    "## Do not forget to rename 'ProductId' below to 'ProductName' when completing the DIY section.\n",
    "recommend_df.rename(columns={'ProductId': 'recommended'},inplace=True)\n",
    "\n",
    "# BatchWrite to DynamoDB customers table.\n",
    "df_customer_ids = recommend_df.drop(['recommended'],axis=1)\n",
    "put_df(df_customer_ids,table=\"customers\")\n",
    "put_df(recommend_df,table='customers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "vscode": {
   "interpreter": {
    "hash": "f2380559e0af56354262e1ca2c8f5b32ce2177e49ec12a71b1ae01e5d359f27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
